\documentclass[letterpaper,12pt,onecolumn,final]{report}

\pdftrailerid{}
\pdfsuppressptexinfo15
\pdfminorversion=4

%% MANDATORY PACKAGES
\usepackage{cuthesis}         % Concordia's thesis style
\usepackage[english]{babel}   % load english localization
\usepackage{type1ec}          % type 1 font
\usepackage[T1]{fontenc}      % correct some font representation, needs cm-super fonts
\usepackage{times}            % use Times New Roman font
\usepackage[titletoc,title]{appendix}     % include Appendix command, add to ToC
\usepackage{setspace}        

%% OPTIONAL PACKAGES
%\counterwithout{footnote}{chapter}        % do no reset footnote # between chapters
\usepackage[hyphens]{url}     % print links
\usepackage{hyperref}         % provides hyperlinks (text different than link)
%\usepackage[hyphenbreaks]{breakurl}       % break long URL after hyphens
\hypersetup{
	colorlinks=true,
	breaklinks=true,
	linkcolor=black,
	citecolor=black,
	urlcolor=black,
	filecolor=black,
	linktoc=all,
}
\usepackage{graphicx}
%\graphicspath{{img/}}

\usepackage{blindtext}


%% CUSTOM MACROS
%\newcommand{\tickyes}{{\small\checkmark}}
%\newcommand{\tickno}{{\small$\times$}}

%% CUSTOM COMMANDS
%\newcommand{\subhead}[1]{\noindent{\textbf{#1.}}}

%% THESIS SETTINGS
\author{Mosabbir Khan Shiblu}
\title{Refactoring Detection in JavaScript}

% As of 2019, title is no longer used...
%\titleOfPhDAuthor{Mr.}         % or Ms., Mrs., Miss, etc. (only for PhD's)

% if PhD, uncomment:
%\PhD
% else if Master's, uncomment:
\mastersDegree{Master of Computer Science}
\program{Computer Science}
\dept{The Department\\of\\Computer Science and Software Engineering}

%% See current GPD at https://www.concordia.ca/admissions/graduate/programs/contacts.html
\GpdOrChairOfDept{Dr.\ 	LEILA KOSSEIM}
\isGpd % Chair by default
%% See current Dean at  https://www.concordia.ca/ginacody/about/leadership/office-dean/dean-of-engineering-and-computer-science.html
\deanOfENCS{Dr.\ Mourad Debbabi } 
\chairOfCommittee{Dr.\ Chair}
\examinerExternal{Dr.\ External}
\examinerFirst{Dr.\ Examiner1}
\examinerSecond{Dr.\ Examiner2}
\examinerExternalToProgram{Dr.\ ExternalToProgram}
\supervisor{Dr.\ Nikolaos Tsantalis}
%% Following two lines are required if you have a co-supervisor
%\hasCosupervisor
%\coSupervisor{Dr.\ Co-supervisor}

%% Comment to use current month, needs to match initial submission
\submitmonth{November}
\submityear{2021}
%% Comment if date of defence is unknown yet, fill for final submission
%\defencedate{December 7, 2021}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\doublespacing
\begin{document}

\begin{abstract}
{%trick to force double spacing in the abstract, otherwise some paragraphs may show single %spaced
\setstretch{1.6667}

Refactoring refers to any code changes that improve the maintainability of the software system. Identifying such activities helps understanding the evolution and the relationship between two versions of a system. Therefore, automatic detection of refactorings applied in a system by comparing the source code between two snapshots has been an active research topic. Current state of the art refactoring detection tools RefactoringMiner 2.0, however only supports programs written in Java language. On the other hand, JavaScript, despite being the most popular language, is supported by only one refactoring detection tool - RefDiff 2.0 which cannot detect function level refactorings such as rename variable, rename parameter etc. 

In this study we present JSRMiner which supports \textbf{16} different refactoring operations including several function level refactorings in JavaScript projects . Although the tool is inspired by RefactoringMiner it differs quite a lot from refactoring miner in terms of structure. We evualated JSRMiner against an oracle of \textbf{300} refactoring instances mined from 10 open source javascript projects and compared with RefDiff 2.0. Our results indicate that JSRMiner can achieve satisfactory precision although cannot beat RefDiff 2.0.

}\end{abstract}


%\doublespacing
\begin{acknowledgments}

% keep this section even if empty
	%\blindtext[2]
I would like to express my gratitude and thanks to my supervisor, Dr. Nikolaos Tsantalis. His invaluable guidance and continuous support opened a new horizon of knowledge to me.

I would also like to thank my colleagues, Mohammad Sadegh Aalizadeh, Mehran Jodavi, and Ameya Ketkar who shared their experiences and were amazing in teamwork and helped me to learn a lot in my journey at Concordia.

\vspace{5mm}

Thank you.

Mosabbir Khan Shiblu
	
\end{acknowledgments}


%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\section{Motivation}

Refactoring activities plays an important role in the modern software development life cycle. In addition to referring to source code changes that improve its maintainability, refactoring is frequently used to denote changes that improve software performance, software security, and even the energy consumption of a system. In an agile environment, it enables the limited upfront design of the software to advance. On the other hand, for test-driven environment, it is regarded as a necessary activity in keeping the code-base compliant for further development. 

A recent survey paper \cite{abid202030} reported over 3,000 papers on refactoring topics which asserts its popularity in modern research. Many researchers empirically investigated the benefits of refactorings by studying how the renaming of identifiers affects code readability \cite{fakhoury2019improving}, how and why developers rename identifiers \cite{peruma2018empirical}, the impact of refactoring on code naturalness \cite{lin2019impact}, the impact of refactoring on code smells \cite{cedrim2017understanding}, the co-occurrence of refactoring and self-admitted technical debt removal \cite{iammarino2019self}, and how the introduction of Lambda expressions affects program comprehension \cite{lucas2019does}.

Therefore, by detecting refactorings in software projects, researchers can better understand software evolution. Earlier studies used such information to reveal the usage of refactoring tools  \cite{MurphyHill2012}, \cite{Negara2013}, the motivations behind refactoring \cite{kim2012field}, \cite{kim2014empirical}, \cite{Silva2016}, the risks associated with refactoring \cite{kim2012field}, \cite{kim2014empirical}, \cite{kim2011empirical}, \cite{weissgerber2006refactorings}, \cite{bavota2012does}, and the effect of refactoring on code quality metrics \cite{kim2012field}, \cite{kim2014empirical}. Additionally, the accuracy of source code evolution analysis can be improved by keeping track of recordings because files, classes, or functions may have their histories split by refactorings such as Move or Rename \cite{hora2018assessing}. Lastly, according to a survey spanning 86 articles \cite{soetens2017changes}, most desirable future application for detecting changes that occurred between two program versions are detecting patterns of change and re-performing changes in different contexts.

Knowing the applied refactoring operations in the version history of a system (i.e. commit) not only can help advancing software evaluation research but also can be help developers in their practice.  First, such information can be used to help resolving merge conflicts and improve code review time as many developers face difficulties when reviewing or integrating code changes with large refactoring operations \cite{kim2012field}. Ironically it has been reported that refactoring activities can cause merge conflicts when merging development branches \cite{mahmoudi2019refactorings}. Therefore, if a tool can identify applied refactoring on commit level, it can possibly be used to resolve merge conflicts automatically. Second, identified refactoring instances can be automatically append in the commit message to let code/reviewers know about the refactored components upfront. Third, if an API is refactored, corresponding refacotirngs could be applied on the consumer code automatically \cite{henkel2005catchup} \cite{Xing2008}. Forth, detected refactorings can be used to separate  new lines of codes representing a feature in a software development sprint which can potentially help a project manager to monitor the progress of the current milestone. Last but no the least, refactoring detection tools can potentially be be used to increase the accuracy of copy-right violation detection tools.

Detecting refactoring is a challenging task due to the fact that developers rarely documents such activities \cite{alomar2021refactoring} and refactorings are often bundled with other code changes making them even harder to isolate from source code. However, given the practical importance of refactoring in software development as well as its research potential, it is unsurprising that we have seen many automatic refactoring detection tools over the past few decades. RefactoringMiner 2.0 by Tsantalis et. al \cite{Tsantalis2020} currently represents the state of the art and is capable of detecting 40 refactorings types with a precision and recall of 99.6\% and 94\% respectively.  Unfortunately like majority of the detection tools it supports only Java programming language. On the other hand, REFDIFF 2.0 by Silva et. al  \cite{Silva2020} is the only tool capable of detecting refactorings in JavaScript project. RefactoringMiner structurally match statements of code thus capable of detecting lower level refactorings (such as rename /merge variable). On the other hand, the body of a function is represented as bag of token by RefDiff and thus such lower level structural information is lost after the tokenization.

Given the fact that JavaScript is currently the most popular programming language, a refactoring detection tool which can detect lower-level refactorings in JavaScript may provide more insight of its ecosystem. as developers tend to apply such refactorings more frequently than high-level refactorings \cite{MurphyHill2012}. Besides, there are significant differences between Java and JavaScript beside language grammar, For example, in JavaScript,  functions are the first class citizen and can be stored and used like a variable. JavaScript code can reside outside of the body of a function. Moreover, unlike Java which is object pretend and class based, JavaScript projects can follow styles of OOP, functional programming or a mix of both. Lastly, there are many  super set and extensions of JavaScript language (e.g. Typescript JSX) which are often inter-mixed with vanilla JavaScript code further complicating the task of parsing and modeling the source code.

In this thesis, we present JsrMiner - the first tool to detect method level refactorings in JavaScript projects. Although this tool is heavily inspired by RefactoringMiner 2.0 it does not represent or reflect the accuracy and efficiency of Refactoring Miner 2.0 anyway. To evaluate the performance of JsrMiner, we ran it on 10 open source JavaScript projects, and manually validated the refactoring detection. We also compared JsrMiner with the only publicly available refactoring detection tool for JavaScript, namely RefDiff 2.0. On average, JsrMiner achieves a precision of \_\% and a recall of \_\%. Although the precision is higher than the current state-of-the-art, JsrMiner has performed poorly on recall.  This result showed the potential of JsrMiner for some specific refactoring detection in JavaScript projects.

\section{Thesis Statement}

TODO

\_ RQs
\_Description of RQs

\section{Objectives and Contributions}
In this thesis, we set out to finding method level refactorings in JavaScript project using a approach inspired by RefactoringMiner 2.0. This approach was embodied in our tool JsrMiner and compared against the current state of the art Refdiff 2.0. In contrast to RefDiff 2.0, our approach does not rely on any similarity threshold which makes it suitable for detecting refactorings between any two source code that could has significant textual difference. Moreover, our tool is the only tool that can detect method-level refactorings. 


This thesis makes the following contributions:

(1) We presented the first tool to detect method-level refactorings applied between two version of JavaScript programs. 
(2) Our tool JsrMiner can detect a total of 19 refactorings
(3) We compare JsrMiner with the current state-of-the-art refactoring detection tool: RefDiff 2.0, where an empirical study on 100 commits from 10 open source JavaScript repositories including 3000 true refactorings are performed.
(4) \_Efficiency, precision, recall


\section{Outline}
\label{sec:outline}
The rest of the thesis is structured as follows. \hyperref[chap:relatedWork]{Chapter 2}  provides an overview and discussion of related works. Our approach to automatic detection of the refactorings that occurred between two program versions is presented in Chapter 3. The correctness and completeness of our approach is evaluated in Chapter 4, while the limitations of the proposed approach and threats to the validity of our study are discussed in Chapter 4 respectively. Finally, in Chapter 5, we provide our conclusions and discuss possible related future research.



%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
\label{chap:relatedwork}

In this section, we start by briefly talking about the the spectrum of research exploring the practice of refactoring and later go into details about modern refactoring detection tools.

Leo Brodie \cite{thinkingforth} first mentioned the word “Refactoring” in his book “Thinking Forth”, originally published in 1984. As per the author, "Factoring" and "Refactoring" were interchangeably used in the "Forth community" back then and he defined refactoring activities as identifying useful fragments that could be pulled out to make code more generally useful and more maintainable, or to eliminate duplication. In addition, the author also discussed many software development principles and practices that are still applicable to date.

However, it was Opdyke \cite{OPDYKE1990} who generalized refactorings as any source code transformations that improves the understandability and re-usability of source code.

In an early work, by Mens and Tourwe \cite{Mens2004}, an overview of the existing research was provided in terms of refactoring practices and techniques, refactoring tools, and the effect of refactorings on the software process. Further studies studied the impact of refactoring on code quality \cite{Moser2006} \cite{wilking2007empirical} \cite{bavota2015experimental} \cite{cedrim2016does} \cite{pantiuchina2018improving} \cite{alomar2019impact},  detecting refactoring opportunities \cite{fontana2012automatic} \cite{palomba2013detecting}, refactoring recommendations, \cite{mkaouer2015many}; \cite{bavota2014recommending} \cite{ouni2016multi}, and automated refactoring tools \cite{roberts1997refactoring} \cite{mazinanian2016jdeodorant} \cite{Kim2010} \cite{Tsantalis2018} \cite{Silva2020} \cite{Moghadam2021}.

\section{Refactoring Detection Approach}
\subsection{Detection Using Meta Data}
Several studies have used metadata such as commit messages from version control systems to detect refactorings. 

Ratzinger et al. \cite{ratzinger2008relation} searched for a predefined set of terms (e.g. ”refactor") in commit messages to classify them as refactoring changes. Kim et al. \cite{kim2014empirical} reported that in some cases, a branch may be created exclusively to refactor the code. Soares et al. \cite{soares2010making} proposed an approach that can detect behavior-preserving changes by automatically generating and running test-cases and can also be employed to classify behavior-preserving commits. Recently, Krasniqi and Cleland-Huang \cite{Krasniqi2020} implemented CMMiner that is capable of detecting 12 refactoring types based on analysing commit logs provided by developers.

\subsection{Detection By Tracking IDE Activities}
Murphy-Hill et al. \cite{MurphyHill2012} tracked the usage history of refactoring commands available in Eclipse IDE using a plugin and found that developers had performed about 90\% of their refactorings manually instead of opting for the refactoring tool. Additionally, they often interleave refactorings with other behavior-modifying programming activities. Furthermore, developers rarely explicitly report their refactoring activities in commit messages.

Negara et al. \cite{Negara2013} developed CODINGTRACKER which infers refactorings from continuous code changes with the help of a refactoring inference plugin. Using their tool, they constructed a large corpus of 5,371 refactoring instances performed by 23 developers working on their IDEs. Their approach reported precision and recall of 93\% and 100\% respectively for a sample of both manually and automatically performed refactorings.

Similar to CODINGTRACKER \cite{Negara2013}, GHOSTFACTOR \cite{Ge2014} and REVIEWFACTOR \cite{Ge2017} infer fully completed refactorings by monitoring the fine-grained code changes in real-time inside the IDE. On the other hand BENEFACTOR \cite{Ge2012} and WITCHDOCTOR \cite{Foster2012} offer code completion by detecting ongoing manual refactorings.

\subsection{Detection by Static Source Code Analysis}
Static analysis is a widely popular and modern approach for finding differences between two versions of a software system. It has the advantage of being able to detect applied refactoring  from software version histories. As our tool falls into this category, in this section , we will go in depth about existing static refactoring detection tools.

Demeyer et al. \cite{Demeyer2000} introduced the first strategy for identifying the refactored elements between two system snapshots. They defined four heuristics based on the changes of object-oriented source code metrics such as method size, class size, number of inherited or overwritten methods to identify refactorings of three general categories (Split/Merge Class, Move Method, and Split Method). For example, to detect Extract Superclass refactoring, they start by inspecting the increase in the inheritance hierarchy of a class to detect newly added classes. Then they observed whether the number of methods and fields in the hierarchy has been decreased but increased in the newly added class. To validate their technique, they applied it on different versions of three software systems. However, the precision of their approach was seemingly on the lower side, for example, for Move Method refactorings (limited to super, sub, and sibling classes) the reported average precision was 23\%. One of the reasons for low accuracy is due to the partial overlapping of heuristics causing some false negative refactorings to be reported as false positives for other refactorings. On the other hand, the paper concluded that from the perspective of reverse engineering, the proposed heuristics were extremely useful to uncover where, how, and maybe why implementation had drifted from its original design.

Antoniol et al. \cite{Antoniol2004} used an automatic technique based on Vector Space cosine similarity to compare identifiers in different classes in order to detect the renaming and splitting of classes. Since it's based on a similarity threshold, it does not perform very well for classes with many changes and may require threshold adjustment on a case basis.

Weißgerber and Diehl \cite{Weissgerber2006} developed the first technique for identifying class-level/locally-scoped refactorings i.e refactorings that occur within one class, and thus, within the same file (e.g. Rename Method). Their approach first extracts and identifies added and deleted refactoring candidates (fields, methods, and classes) by parsing deltas and then comparing each pair's name similarity from a version control system. For ambiguous candidate pairs, it uses a clone detection tool CCFINDER \cite{Kamiya2002} to compare their bodies and then rank them. CCFINDER is also configured to ignore whitespaces/comments and to match consistently renamed variables, method names, and references to members. Finally, they used random sampling to estimate the precision whereas commit messages were inspected manually to find documented refactorings in order to compute the recall. Although they have achieved a good recall of 89\%, it has been proven that commit messages are not reliable indicators of refactoring activity \cite{Krasniqi2020}, \cite{MurphyHill2012}. Lastly, the authors stated that their technique is susceptible to multiple refactorings performed on the same entity.

Dig et al. \cite{Dig2006} developed an Eclipse plug-in named REFACTORINGCRAWLER which initially uses a computationally inexpensive text-based similarity metric - Shingles encoding \cite{Broder1997} to find possible refactoring candidates. Shingles act as "fingerprints" for texts (e.g. method bodies) and reduce the ramification of small textual changes like renaming and minor edits. This enables REFACTORINGCRAWLER to detect similar pairs of high-level code elements (methods, classes, and packages) between two versions of a project much more robustly than existing string matching techniques that are vulnerable to minor changes. To detect actual refactorings, it then refines the candidates by employing a more expensive and precise semantic analysis based on reference graphs. To evaluate the performance of REFACTORINGCRAWLER, it was applied on three open-source Java projects and archived high values for both precision(95\%) and recall(90\%). However, similar to the work of Weißgerber and Diehl \cite{Weissgerber2006} the authors manually discovered the applied refactorings by inspecting their commits/ release notes for computing recall while they inspected the source code to compute precision. Later, Biegel et al. \cite{Biegel2011} replicated Weißgerber’s approach using three different similarity metrics: CCFinder (text-based), JCCD \cite{Biegel2010} (ast-based), and Shingles (token-based). It was concluded that the three metrics performed with a comparable quality even though they can affect the ranking of refactoring candidates.

Xing and Stroulia \cite{Xing2006} used both textual and structural similarity to detect refactorings between two versions of a system in their Eclipse plug-in named JDEVAN \cite{Xing2007} \cite{Xing2008}. JDEVAN initially constructs two UML logical design models from the source code corresponding to two versions of a Java system.  Next, using UMLDiff \cite{Xing2005}, the two models are compared and the differences between them are reported as removal, addition, moving, and renaming of UML entities (e.g. class, package). Finally, JDEVAN’s refactoring-detection module defines a suite of queries \cite{Xing2006} assisted by a set of similarity metrics that attempt to categorize detected differences as refactoring instances through a hierarchical pairwise comparison between the two models' packages, classes, methods, and fields. As an example of an implemented query, an \textit{Extract Operation} refactoring is inferred when the set of usage relations (read, write, call, instantiate) inside a newly added method proved to be a subset of the removed usage relations from the original method or their intersection set is greater than a user-specific threshold.  JDEVAN found all the documented refactorings when applied on two systems and proved to be useful in detecting different types of refactorings in several studies. However, the authors confirmed that its rename and move refactorings detection is vulnerable to cases where there are not enough relations between the refactored entities and other parts of the program.

REF-FINDER \cite{Kim2010} by Pete et al. \cite{Prete2010} is capable of detecting 63 of Fowlers' catalog \cite{Fowler1999} of 72 refactoring types which contains the most comprehensive list of refactoring types to date. It first describes classes, methods, and fields as a set of logic predicates along with their content (e.g. method body) and structural dependencies (i.e. field access, method calls, subtyping, and overriding) to represent the versions of a program as a database of logic facts. Additionally, supported refactorings are encoded as logic rules where the antecedent defines the constraints (i.e. changefacts) and the consequent holds the refactoring type to be inferred. Next, it converts the antecedent of these logic rules as logic queries and then invokes them against the database of logic facts to identify program differences that match the constraints of each refactoring type under focus. Besides, by tracking the dependencies among refactoring types, lower-level refactorings were queried to identify higher-level, composite refactorings making Ref-Finder the first tool capable of detecting composite refactorings where each refactoring consists of a set of atomic refactorings. For example, Extract Superclass refactoring is inferred by checking if a new superclass is created and a number of PullUp Method/Field refactorings were identified that had moved fields and methods to the newly created class. For the detection of some types of refactoring, their rules require a special logic predicate that indicates if the word-level similarity between two candidate methods is above a threshold. This was implemented as a block-level clone detection technique that trims parenthesis and removes escape characters, returns keywords, and computes the world-level similarity between the two code fragments using the longest common subsequence algorithm. The tool was tested on three open-source Java programs and precision of 95\% and recall of 79\% were reported. However, later studies reported lower precision and recall for Ref-Finder \cite{Soares2013} \cite{Silva2017} \cite{Tan2019}. Considering refactorings applied on isolation (root canal refactorings \cite{MurphyHill2012}) and ignoring refactorings with overlapping changes (i.e. floss refactorings \cite{MurphyHill2012}) was the reason behind higher precision and recall in the evaluation conducted by the authors.

Now we are going to discuss the current state of the refactoring detection tools RefDiff \cite{Silva2020}, RefactoringMiner 2.0 \cite{Tsantalis2020} and RefDetect \cite{Moghadam2021} which are closely related to our work.

\subsubsection{RefactoringMiner}
Tsantalis et al. \cite{Tsantalis2013} proposed a tool based on an extended, lightweight variation of the UMLDiff \cite{Xing2005} which is an algorithm for differencing object-oriented models. It is capable of identifying 14 high-level refactoring types: Rename Package/Class/Method, Move Class/Method/Field, Pull Up Method/Field, Push Down Method/-Field, Extract Method, Inline Method, and Extract Superclass/Interface. The process identifies refactorings between two models in several rounds. First, it compares the names or signatures of classes, methods, and fields in a top-down fashion and determines whether they have been matched, removed from the first model, or added to the second model. Next, removed elements are compared against the added elements by the equality of their names and parameter count to identify the changes in signatures of fields and methods. Third, the leftover removed/added classes are matched based on the similarity of signatures of members from the previous step thus this step can endure type changes. Finally, a set of refactoring detection rules defined by Biegel et al. \cite{Biegel2001} was extended and employed to infer actual refactoring instances. To evaluate their approach, the authors applied their technique in the version histories of three projects and reported 96.4\% precision for Extract Method refactoring with 8 false positives and 97.6\% precision for Rename Class refactoring with 4 false-positive instances. No false positives were found for the remaining refactorings. Later, Silva et al. \cite{Silva2016} extended and re-introduced the tool as Refactoring Miner and used it to mine refactorings on large scale in git repositories. In their evaluation of the tool, a precision of 63\% with 1,030 false positives out of 2,441 refactorings was reported. On the other hand, Refactoring Miner achieved precision and recall of 93\% and 98\% respectively when the authors evaluated it as a benchmark on the dataset created by Chaparro et al. \cite{Chaparro2014}.

\subsubsection{RefDiff 1.0}
In their tool REFDIFF, Silva and Valente \cite{Silva2017} introduced the concept of analyzing only the changed, added, or deleted files between two versions of a program to detect refactorings. Its is capable of detecting 13 high-level refactoring types through static analysis and code similarity comparison. As a first step, Ref Diff represents the body of classes and methods as a multiset (or bag) of tokens whereas for each field it considers tokens of all the statements that use that field. Next, to find similarity between code entities, a variation of the TF-IDF weighting scheme \cite{Salton1986} is used to assign more weight to tokens that are less frequent, and thus have finer distinctive importance than other tokens. Additionally, the similarity threshold for different kinds of code elements is calibrated by using a set of ten commits from ten different open-source projects for which the project developers themselves have confirmed the applied refactorings \cite{Silva2016}. Finally, similar to the evaluation performed by Perte et al. \cite{Prete2010}, they evaluated REFDIFF based on an oracle of root canal refactorings applied by graduate students in 20 open-source projects. The evaluation suggested that REFDIFF surpassed RMiner \cite{Silva2016}, RefactoringCrawler \cite{Dig2006} and Ref-Finder \cite{Prete2010} in terms of performance and accuracy.

\subsubsection{RefactoringMiner 1.0 /RMiner}
Later, Tsantalis et al. \cite{Tsantalis2018} proposed a major evolution of their existing RefactoringMiner \cite{Silva2016} \cite{Tsantalis2013} tool and renamed it to RMiner (RefactoringMiner version 1.0) which is the first refactoring detection tool that does not rely on code similarity thresholds. Similar to REFDIFF, RMiner also processes only the change, added, or deleted files of a commit however, unlike its competitors such as REFDIFF, REF-FINDER, UMLDIFF, and REFACTORINGCRAWLER; RMiner can tolerate unparseable programs. Consequently, this technique can achieve better accuracy and efficiency by decreasing the number of incorrect code entity matches as the majority of the change history of software systems cannot be successfully compiled \cite{Tufano2017}. RMiner employs an AST-based statement matching algorithm and a set of detection rules to detect 15 representative refactoring types. It matches statements in a round-based fashion where textually identical statements are matched first. Then, the algorithm employs two novel techniques: abstraction, to facilitate the matching of statements having a different AST node type and argumentation, which deals with changes in sub-expressions within statements due to the replacement of expression with method parameters, and vice-versa. To deal with overlapping refactorings (e.g. variable renames), while matching two statements, RMiner performs a syntax-aware replacement of the compatible AST nodes to make them identical. For evaluation, the authors created a dataset with 3,188 real refactorings instances from 185 open-source Java projects. Using this oracle, the authors reported a precision of 98\% and recall of 87\%, which was the best result so far, surpassing RefDiff \cite{Silva2017}, the previous state-of-the-art, which achieved a precision of 75.7\% and a recall of 85.8\% in this dataset. The superiority of RMiner is also confirmed by Tan and Bockisch \cite{Tan2019} where it emerges as the winner among its competitors: RefactoringCrawler \cite{Dig2006}, Ref-Finder \cite{Prete2010} and RefDiff \cite{Silva2017}.


\subsubsection{RefDiff 2.0}
In continuation to their previous work, Silva et al. \cite{Silva2020} upgraded REFDIFF \cite{Silva2017} and represented as REFDIFF 2.0 \footnote{https://github.com/aserg-ufmg/RefDiff} which is the first multi-language refactoring detection tool. The tool is capable of detecting refactorings in Java, C, and JavaScript programs and remains the only known tool capable of detecting performed refactorings in JavaScript. It employs a two-phase approach where in the first phase source codes are represented as Code Structure Tree (CST) that abstracts away the detail of a particular language. Each node in CST is represented by higher-level entities such as classes, functions, etc. Since code can be written outside of a class in JavaScript and C, files are also considered as CST nodes. In the second phase, REFDIFF 2.0, uses the same approach as its predecessor and determines the similarity between the CST nodes by tokenizing the body of CST nodes and then computing their weight using a variation of the TF-IDF weighting scheme. However, in contrast to its predecessor, REFDIFF 2.0 uses a single default similarity threshold of 0.5 for all kinds of code element relationships. For Java projects, REFDIFF 2.0, was evaluated on the same oracle \cite{Tsantalis2018} \footnote{http://refactoring.encs.concordia.ca/oracle} that was used to evaluate RMiner \cite{Tsantalis2018} and a precision of 96\% and a recall of 80\% were reported. On the other hand, for C projects, RefDiff 2.0 achieved a precision of 88\% and a recall of 91\% based on a small-scale experiments. For JavaScript, the computed precision was 91\% using 87 refactorings and recall was 88\% using 65 refactoring instances. Later Brito and Valente \cite{Brito2020} created a GO language plugin of RefDiff 2.0 named RefDiff4Go and reported similar precision (92\%) and recall (80\%) based on six GO projects.

\subsubsection{RefactoringMiner 2.0}
Recently, Tsantalis et al. extended RMiner \cite{Tsantalis2018} and introduced RefactoringMiner 2.0\footnote{https://github.com/tsantalis/RefactoringMiner} \cite{Tsantalis2020}. The tool is capable of detecting over 80 different types of refactoring operations including low-level ones that occur in the method body (e.g., Inline/Extract/Split/Rename Variable) in Java projects. The main improvement is in the matching function, where new replacement types and heuristics are added. In evaluation, the authors compare their tool with existing tools including its predecessor RefactoringMiner 1.0/ RMiner \cite{Tsantalis2018}, and RefDiff 2.0 \cite{Silva2020} by using a dataset containing 7,226 true instances for 40 different refactoring types, which are validated by experts. The results proved the superiority of the new version of RefactoringMiner 2.0 by achieving the best precision (99.6\%) and recall (94\%) from the tools evaluated. Zarina \footnote{https://github.com/onewhl} from JetBrains-Research \footnote{https://github.com/JetBrains-Research/kotlinRMiner} leads and maintains kotlinRMiner which is essentially an extension for Refactoring Miner 2.0. Additionally, Python extensions are also created by Atwi et al. \cite{Atwi2021} and Dilhara et. al. \footnote{https://github.com/maldil/RefactoringMiner}.

\subsubsection{RefDetect}
In contrast to RefDiff 2.0 \cite{Silva2020} which uses token similarity and RefactoringMiner 2.0 \cite{Tsantalis2020} which structurally matches the code fragments, RefDetect by Moghadam et al. \cite{Moghadam2021} employs a completely different approach where the whole program is represented as a sequence of characters that abstracts away the specifics of that particular language. Their approach detects refactorings in three steps. First, each entity is represented by 7 different types of characters in a specific order: class (C), interface (I), generalization/inheritance relationship (G), attribute (A), method (M), method parameter (P), and a property access/coupling relationship between two classes as (R). As an example, if a class B has two methods, inherits class A, and accesses a field of another class C, B is represented as CGMMR. These entities representing sub-strings are sorted by the original name of the corresponding entity and form a single string that represents one version of the input program. Second, a sequence alignment algorithm (FOGSAA \cite{chakraborty2013fogsaa}) is used to identify the changes existing between the two input program versions. For each pair of characters in the input strings, the alignment algorithm considers three possibilities: match, mismatch, or gap and returns the initial list of unmatched entities. Third and the final step, a threshold-based refactoring detection algorithm is used to identify the set of applied refactorings that resulted in the evolution from the older version to the newer one. RefDetect was evaluated and compared with the current state-of-the-art refactoring detection tool RefactoringMiner 2.0 on the same aforementioned publicly available refactoring oracle. The authors found that while RefactoringMiner 2.0 clearly outperforms RefDetect in terms of precision (98.5\% vs 91.2\%), RefDetect achieved a better recall (84.5\% vs 78.9\%).

\section{Limitations of Existing Approaches}

We will now discuss some of the key limitations of existing approaches that we tried to overcome in our proposed approach.

\textbf{Dependence on similarity thresholds:} Most of the refactoring detection tools use similarity metrics to compute the resemblance between a pair of code elements originating from two different snapshots of a software system. These metrics typically require user-calibrated thresholds to determine whether the elements should be considered as matched. Since developers often perform other maintenance activities (e.g. bug fixes, performance) during refactoring \cite{MurphyHill2012}, similarity thresholds often help endure such changes. Modern refactoring detection tools comes pre-configured with default thresholds that are empirically derived through experimentation on a relatively small number of project (one for UMLDIFF, three for REF-FINDER, and REFACTORINGCRAWLER, and ten for REFDIFF 2.0). Therefore such thresholds risk being over-fitted to the test projects thus cannot be general enough to handle all the possible ways refactorings are applied in projects from different domains. Consequently, it may require a manual inspection of the reported refactorings against the source code to find false positives in order to re-calculate the thresholds.

Several studies in the field of software measurement and metric-based code smell detection, extensively investigated the problem of deriving the holy grail value for a particular threshold by applying various statistical methods and machine learning techniques on a large number of software projects \cite{alves2010deriving}, \cite{ferreira2012identifying}, \cite{oliveira2014extracting}, \cite{fontana2015automatic}, \cite{fontana2016comparing}. Dig et al. \cite{dig2006automated} reported that precision and recall can vary significantly for the same software system based on different values of thresholds. Moreover, Aniche et al. \cite{aniche2016satt} has shown that different threshold values are required for source code metrics for software systems using different architectural styles and frameworks. This is especially true for JavaScript programs where unlike the object-oriented style of Java, developers can opt for either functional or object-oriented fashion or a mix of both and the threshold yielding better results in object-oriented programming may need adjustment for projects written in a functional approach. Therefore, based on experience, it can be concluded that it is very difficult to derive universal threshold values that can work well for all projects, regardless of their architecture, domain, and development practices \cite{Tsantalis2020}.

\textbf{Dependence on built project versions} Tools like RefactoringCrawler, RefFinder and UMLDiff requires building the project versions in order to retrieve structural information such as field access, method call, subtyping and overriding etc. To accurately obtain such information in Java projects, a compiler such as eclipse JDT plugin is used to resolve type, variable and method binding information. However this approach is severely limited by the fact that most commits are not compilable \cite{Tufano2017}. As a result, these tools may not be suitable for performing large-scale refactoring detection in the entire commit hisoty of a project.

\textbf{Incomplete oracle:} Calculating the true recall for refactoring detection studies has always been very challenging and debatable since it is hard to identify all the refactorings have been performed in commit. Inspecting commit messages has been proved to be an unreliable indicator of refactorings in a commit as developers do not always mention them in the commit message \cite{MurphyHill2012}. Moreover, Moreno et al. \cite{laura2017arena} found out that only 21\% of the release notes include information about refactoring operations by manually inspecting 990 release notes from 55 open source projects. Last but not the least, analyzing each file manually for identifying performed refactorings is time intensive and requires expertise and does not guarantee of finding all the refactorings. 

\textbf{Java Based Literature:} Majority of the existing approaches support only Java systems. In fact, to the best of our knowledge no other tool can detect refactorings in JavaScript ecosystem except RefDiff 2.0. Even so, because of the nature of its detection methodology, RefDiff 2.0 cannot detect method level refactorings such as rename variable, add parameter etc.

%\paragraph{SDsadsa}

%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Then}


%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion and future work}
\label{chap:conclusion}

TODO
%\blindtext[5]

%This text requires a citation \cite{thinkingforth} \cite{MurphyHill2009} to embed the citation in the required position in the text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}  %  Add Bibliography to TOC
\singlespacing % save space in the bibliography
\bibliographystyle{unsrt}
\bibliography{references}



%%%%%%%%%% Appendices %%%%%%%%%%%%%%%%
% ---- Appendix settings. Please Do NOT change them. -----
\appendix
\setcounter{table}{0}		% reset the table counter
\setcounter{figure}{0}		% reset the figure counter
\renewcommand{\thefigure}{\Alph{chapter}.\arabic{figure}} 	% numbering the a figure in Appendix as Figure A.2, Figure B.1, etc.
\renewcommand{\thetable}{\Alph{chapter}.\arabic{table}}		% numbering the a table in Appendix as Table A.2, Table B.1, etc.

%%%%%%%%%% Body of Appendix %%%%%%%%%%%%%%%%
\begin{appendices}
\doublespacing

\chapter{First Appendix}
\label{chap:apdx1}

\blindmathpaper

\chapter{Concordia Logos}
\label{chap:logos}
\begin{figure}[h!]
	\centering
	\includegraphics{logos/Concordia_University_logo}
	\caption{Concordia University}
\end{figure}
\vspace{2em}
\begin{figure}[h!]
	\centering
	\includegraphics{logos/Concordia_GinaCody_vertical}
	\caption{Gina Cody School of Engineering and Computer Science (vertical)}
\end{figure}
\vspace{2em}
\begin{figure}[h!]
	\centering
	\includegraphics{logos/Concordia_GinaCody_horizontal}
	\caption{Gina Cody School of Engineering and Computer Science (horizontal)}
\end{figure}

\end{appendices}

\end{document}